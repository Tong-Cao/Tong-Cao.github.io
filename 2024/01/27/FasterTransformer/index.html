<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="keywords" content="Hexo Theme Redefine">
    
    <meta name="author" content="Tong">
    <!-- preconnect -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    
    
        
            <link rel="preconnect" href="https://npm.elemecdn.com" crossorigin>
        
    
    <!--- Seo Part-->
    
    <link rel="canonical" href="http://example.com/2024/01/27/fastertransformer/"/>
    <meta name="robots" content="index,follow">
    <meta name="googlebot" content="index,follow">
    <meta name="revisit-after" content="1 days">
    
        <meta name="description" content="FasterTransformerFaster Transformer 是一个 BERT Transformer 单层前向计算的高效实现，其代码简洁明了，后续可以通过简单修改支持多种 Transformer 结构。目前优化集中在编码器（encoder）的前向计算。底层由 CUDA 和 cuBLAS 实现，支持 FP16 和 FP32 两种计算模式，其中 FP16 可以充分利用 Volta 和 Tu">
<meta property="og:type" content="article">
<meta property="og:title" content="FasterTransformer">
<meta property="og:url" content="http://example.com/2024/01/27/FasterTransformer/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="FasterTransformerFaster Transformer 是一个 BERT Transformer 单层前向计算的高效实现，其代码简洁明了，后续可以通过简单修改支持多种 Transformer 结构。目前优化集中在编码器（encoder）的前向计算。底层由 CUDA 和 cuBLAS 实现，支持 FP16 和 FP32 两种计算模式，其中 FP16 可以充分利用 Volta 和 Tu">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/images/Transformer0.png">
<meta property="og:image" content="http://example.com/images/Transformer2.png">
<meta property="og:image" content="http://example.com/images/FT_Mulhead_st.png">
<meta property="og:image" content="http://example.com/images/FT_dataform.png">
<meta property="og:image" content="http://example.com/images/FT_dataform2.png">
<meta property="og:image" content="http://example.com/images/FT_strideGemm.png">
<meta property="og:image" content="http://example.com/images/FT_FFN.png">
<meta property="article:published_time" content="2024-01-27T07:49:02.000Z">
<meta property="article:modified_time" content="2024-08-15T07:18:47.928Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="transformer">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/images/Transformer0.png">
    
    
    <!--- Icon Part-->
    <link rel="icon" type="image/png" href="/images/nezha.webp" sizes="192x192">
    <link rel="apple-touch-icon" sizes="180x180" href="/images/nezha.webp">
    <meta name="theme-color" content="#A31F34">
    <link rel="shortcut icon" href="/images/nezha.webp">
    <!--- Page Info-->
    
    <title>
        
            FasterTransformer -
        
        Tong&#39;s Blog
    </title>
    
<link rel="stylesheet" href="/css/style.css">

    <link rel="stylesheet" href="//npm.elemecdn.com/hexo-theme-redefine@2.1.3/source/assets/fonts.css">
    <!--- Font Part-->
    
    
    
    

    <!--- Inject Part-->
    
    <script id="hexo-configurations">
    let Global = window.Global || {};
    Global.hexo_config = {"hostname":"example.com","root":"/","language":"en","path":"search.xml"};
    Global.theme_config = {"articles":{"style":{"font_size":"16px","line_height":1.5,"image_border_radius":"14px","image_alignment":"center","image_caption":false,"link_icon":true},"word_count":{"enable":true,"count":true,"min2read":true},"author_label":{"enable":true,"auto":false,"list":[""]},"code_block":{"copy":true,"style":"mac","font":{"enable":false,"family":null,"url":null}},"toc":{"enable":true,"max_depth":3,"number":false,"expand":true,"init_open":true},"copyright":true,"lazyload":true,"recommendation":{"enable":false,"title":"推荐阅读","limit":3,"placeholder":"/images/wallhaven-wqery6-light.webp","skip_dirs":[]}},"colors":{"primary":"#A31F34","secondary":null},"global":{"fonts":{"chinese":{"enable":false,"family":null,"url":null},"english":{"enable":false,"family":null,"url":null}},"content_max_width":"1000px","sidebar_width":"210px","hover":{"shadow":true,"scale":false},"scroll_progress":{"bar":false,"percentage":true},"busuanzi_counter":{"enable":true,"site_pv":true,"site_uv":true,"post_pv":true},"pjax":true,"open_graph":true,"google_analytics":{"enable":false,"id":null}},"home_banner":{"enable":true,"style":"fixed","image":{"light":"/images/test2.jpg","dark":"/images/wallhaven-wqery6-dark.webp"},"title":"Tong's Blog","subtitle":{"text":["不管怎样，别忘了热爱生活就好"],"hitokoto":{"enable":false,"api":"https://v1.hitokoto.cn"},"typing_speed":100,"backing_speed":80,"starting_delay":500,"backing_delay":1500,"loop":false,"smart_backspace":true},"text_color":{"light":"#fff","dark":"#d1d1b6"},"text_style":{"title_size":"2.8rem","subtitle_size":"1.5rem","line_height":1.2},"custom_font":{"enable":false,"family":null,"url":null},"social_links":{"enable":true,"links":{"github":"https://github.com/Tong-Cao","instagram":null,"zhihu":null,"twitter":null,"email":null}}},"plugins":{"feed":{"enable":false},"aplayer":{"enable":false,"type":"fixed","audios":[{"name":null,"artist":null,"url":null,"cover":null}]},"mermaid":{"enable":false,"version":"9.3.0"}},"version":"2.1.3","navbar":{"categories":{"Categories":{"icon":"fa-solid fa-folder","path":"/categories/"}},"tags":{"Tags":{"icon":"fa-solid fa-tags","path":"/tags/"}},"auto_hide":false,"color":{"left":"#f78736","right":"#367df7","transparency":35},"links":{"Home":{"path":"/","icon":"fa-regular fa-house"}},"search":{"enable":false,"preload":true}},"page_templates":{"friends_column":2,"tags_style":"cloud"},"home":{"sidebar":{"enable":true,"position":"left","first_item":"menu","announcement":null,"links":null},"article_date_format":"auto","categories":{"enable":true,"limit":3},"tags":{"enable":true,"limit":3}}};
    Global.language_ago = {"second":"%s seconds ago","minute":"%s minutes ago","hour":"%s hours ago","day":"%s days ago","week":"%s weeks ago","month":"%s months ago","year":"%s years ago"};
  </script>
    
    <!--- Fontawesome Part-->
    <link rel="stylesheet" href="//npm.elemecdn.com/hexo-theme-redefine@2.1.3/source/fontawesome/fontawesome.min.css">
    <link rel="stylesheet" href="//npm.elemecdn.com/hexo-theme-redefine@2.1.3/source/fontawesome/brands.min.css">
    <link rel="stylesheet" href="//npm.elemecdn.com/hexo-theme-redefine@2.1.3/source/fontawesome/solid.min.css">
    <link rel="stylesheet" href="//npm.elemecdn.com/hexo-theme-redefine@2.1.3/source/fontawesome/regular.min.css">
    
    
    
    
<meta name="generator" content="Hexo 6.3.0"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head>


<body>
<div class="progress-bar-container">
    

    
        <span class="pjax-progress-bar"></span>
        <span class="pjax-progress-icon">
            <i class="fa-solid fa-circle-notch fa-spin"></i>
        </span>
    
</div>


<main class="page-container">

    

    <div class="main-content-container">

        <div class="main-content-header">
            <header class="navbar-container">
    
    <div class="navbar-content">
        <div class="left">
            
            <a class="logo-title" href="/">
                
                Tong&#39;s Blog
                
            </a>
        </div>

        <div class="right">
            <!-- PC -->
            <div class="desktop">
                <ul class="navbar-list">
                    
                        
                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class="" 
                                    href="/"  >
                                    
                                        
                                            <i class="fa-regular fa-house"></i>
                                        
                                        HOME
                                    
                                </a>
                                <!-- Submenu -->
                                
                            </li>
                    
                    
                </ul>
            </div>
            <!-- Mobile -->
            <div class="mobile">
                
                <div class="icon-item navbar-bar">
                    <div class="navbar-bar-middle"></div>
                </div>
            </div>
        </div>
    </div>

    <!-- Mobile drawer -->
    <div class="navbar-drawer">
        <ul class="drawer-navbar-list">
            
                
                    <li class="drawer-navbar-item flex-center">
                        <a class="" 
                        href="/"  >
                             
                                
                                    <i class="fa-regular fa-house"></i>
                                
                                HOME
                            
                        </a>
                    </li>
                    <!-- Submenu -->
                    
            

        </ul>
    </div>

    <div class="window-mask"></div>

</header>


        </div>

        <div class="main-content-body">

            

            <div class="main-content">

                
                    <div class="fade-in-down-animation">
    <div class="post-page-container">
        <div class="article-content-container">

            
            
                <div class="article-title">
                    <h1 class="article-title-regular">FasterTransformer</h1>
                </div>
            
                
            

            
                <div class="article-header">
                    <div class="avatar">
                        <img src="/images/nezha.webp">
                    </div>
                    <div class="info">
                        <div class="author">
                            <span class="name">Tong</span>
                            
                                <span class="author-label"></span>
                            
                        </div>
                        <div class="meta-info">
                            <div class="article-meta-info">
    <span class="article-date article-meta-item">
        <i class="fa-regular fa-pen-fancy"></i>&nbsp;
        <span class="desktop">2024-01-27 15:49:02</span>
        <span class="mobile">2024-01-27 15:49</span>
        <span class="hover-info">Created</span>
    </span>
    
        <span class="article-date article-meta-item">
            <i class="fa-regular fa-wrench"></i>&nbsp;
            <span class="desktop">2024-08-15 15:18:47</span>
            <span class="mobile">2024-08-15 15:18</span>
            <span class="hover-info">Updated</span>
        </span>
    

    
    
        <span class="article-tags article-meta-item">
            <i class="fa-regular fa-tags"></i>&nbsp;
            <ul>
                
                    <li>
                        <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a>&nbsp;
                    </li>
                
                    <li>
                        | <a href="/tags/transformer/">transformer</a>&nbsp;
                    </li>
                
            </ul>
        </span>
    

    
    
    
    
        <span class="article-pv article-meta-item">
            <i class="fa-regular fa-eye"></i>&nbsp;<span id="busuanzi_value_page_pv"></span>
        </span>
    
</div>

                        </div>
                    </div>
                </div>
            

            <div class="article-content markdown-body">
                <h1 id="FasterTransformer"><a href="#FasterTransformer" class="headerlink" title="FasterTransformer"></a>FasterTransformer</h1><p>Faster Transformer 是一个 BERT Transformer 单层前向计算的高效实现，其代码简洁明了，后续可以通过简单修改支持多种 Transformer 结构。目前优化集中在编码器（encoder）的前向计算。底层由 CUDA 和 cuBLAS 实现，支持 FP16 和 FP32 两种计算模式，其中 FP16 可以充分利用 Volta 和 Turing 架构 GPU 上的 Tensor Core 计算单元。</p>
<p>整个TransFormer结构如下所示，Faster Transformer 主要实现对编码器的优化，我们主要看左侧的编码器部分：</p>
<p><img lazyload="" src="/images/loading.svg" data-src="/images/Transformer0.png" alt="Transformer0"></p>
<h2 id="多头注意力"><a href="#多头注意力" class="headerlink" title="多头注意力"></a>多头注意力</h2><p>多头注意力的结构如下：</p>
<p><img lazyload="" src="/images/loading.svg" data-src="/images/Transformer2.png" alt="Transformer2"></p>
<p>将注意力块中的层结构展开后，如下所示，左侧为实现对应层所用核函数。</p>
<p><img lazyload="" src="/images/loading.svg" data-src="/images/FT_Mulhead_st.png" alt="Mulhead_st"></p>
<h3 id="一、linear"><a href="#一、linear" class="headerlink" title="一、linear"></a>一、linear</h3><p>linear层的实现分成cubalsGemmEx和add_QKV_bias两个核函数实现，这里将偏置单独拿出来是和后面对数据形状改变合在一起。改变数据形状是为了后面cubalsGemmStridedBatchedEx方便计算。</p>
<p>首先使用cubalsGemmEx来对输入数据进行矩阵乘法计算Q、K、V。这里直接调用cubals库。</p>
<p>要注意cubalsGemmE里数据是列优先存储，通过正常的A、B输入进去会变成<mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -0.452ex;" xmlns="http://www.w3.org/2000/svg" width="8.305ex" height="2.47ex" role="img" focusable="false" viewBox="0 -891.7 3670.6 1091.7"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></g><g data-mml-node="mi" transform="translate(783,413) scale(0.707)"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g></g><g data-mml-node="TeXAtom" data-mjx-texclass="ORD" transform="translate(1330.8,0)"><g data-mml-node="mo"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">、</text></g></g><g data-mml-node="msup" transform="translate(2330.8,0)"><g data-mml-node="mi"><path data-c="1D435" d="M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z"></path></g><g data-mml-node="mi" transform="translate(792,413) scale(0.707)"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g></g></g></g></svg></mjx-container> 所以我们通过<mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -0.186ex;" xmlns="http://www.w3.org/2000/svg" width="12.211ex" height="2.203ex" role="img" focusable="false" viewBox="0 -891.7 5397.2 973.7"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D435" d="M231 637Q204 637 199 638T194 649Q194 676 205 682Q206 683 335 683Q594 683 608 681Q671 671 713 636T756 544Q756 480 698 429T565 360L555 357Q619 348 660 311T702 219Q702 146 630 78T453 1Q446 0 242 0Q42 0 39 2Q35 5 35 10Q35 17 37 24Q42 43 47 45Q51 46 62 46H68Q95 46 128 49Q142 52 147 61Q150 65 219 339T288 628Q288 635 231 637ZM649 544Q649 574 634 600T585 634Q578 636 493 637Q473 637 451 637T416 636H403Q388 635 384 626Q382 622 352 506Q352 503 351 500L320 374H401Q482 374 494 376Q554 386 601 434T649 544ZM595 229Q595 273 572 302T512 336Q506 337 429 337Q311 337 310 336Q310 334 293 263T258 122L240 52Q240 48 252 48T333 46Q422 46 429 47Q491 54 543 105T595 229Z"></path></g><g data-mml-node="mi" transform="translate(792,413) scale(0.707)"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g></g><g data-mml-node="msup" transform="translate(1339.8,0)"><g data-mml-node="mi"><path data-c="1D434" d="M208 74Q208 50 254 46Q272 46 272 35Q272 34 270 22Q267 8 264 4T251 0Q249 0 239 0T205 1T141 2Q70 2 50 0H42Q35 7 35 11Q37 38 48 46H62Q132 49 164 96Q170 102 345 401T523 704Q530 716 547 716H555H572Q578 707 578 706L606 383Q634 60 636 57Q641 46 701 46Q726 46 726 36Q726 34 723 22Q720 7 718 4T704 0Q701 0 690 0T651 1T578 2Q484 2 455 0H443Q437 6 437 9T439 27Q443 40 445 43L449 46H469Q523 49 533 63L521 213H283L249 155Q208 86 208 74ZM516 260Q516 271 504 416T490 562L463 519Q447 492 400 412L310 260L413 259Q516 259 516 260Z"></path></g><g data-mml-node="mi" transform="translate(783,413) scale(0.707)"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g></g><g data-mml-node="mo" transform="translate(2948.4,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="msup" transform="translate(4004.2,0)"><g data-mml-node="mi"><path data-c="1D436" d="M50 252Q50 367 117 473T286 641T490 704Q580 704 633 653Q642 643 648 636T656 626L657 623Q660 623 684 649Q691 655 699 663T715 679T725 690L740 705H746Q760 705 760 698Q760 694 728 561Q692 422 692 421Q690 416 687 415T669 413H653Q647 419 647 422Q647 423 648 429T650 449T651 481Q651 552 619 605T510 659Q484 659 454 652T382 628T299 572T226 479Q194 422 175 346T156 222Q156 108 232 58Q280 24 350 24Q441 24 512 92T606 240Q610 253 612 255T628 257Q648 257 648 248Q648 243 647 239Q618 132 523 55T319 -22Q206 -22 128 53T50 252Z"></path></g><g data-mml-node="mi" transform="translate(845.3,413) scale(0.707)"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g></g></g></g></svg></mjx-container>来得到C。</p>
<div class="highlight-container" data-rel="C++"><figure class="iseeu highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 输入数据排布: [batch_size, seq_len, head_num, size_per_head]</span></span><br><span class="line"><span class="comment">// 把batch_size * seq_len看成行</span></span><br><span class="line"><span class="comment">// 把head_num * size_per_head看成列</span></span><br><span class="line"><span class="comment">// 第一步需要做的是：Q = input_tensor * Q_{param} + Q_{bias}</span></span><br><span class="line"><span class="comment">//                K = input_tensor * K_{param} + K_{bias}</span></span><br><span class="line"><span class="comment">//                V = input_tensor * V_{param} + V_{bias}</span></span><br><span class="line"><span class="comment">// 其中Q_{param}, K_{param}, V_{param}大小都是</span></span><br><span class="line"><span class="comment">// [head_num * size_per_head, head_num * size_per_head]</span></span><br><span class="line"><span class="comment">// 我们把加上bias的部分放到cuda kernel里面去做，这里只做input_tensor * M_{param}</span></span><br><span class="line"><span class="comment">// 也就是(m, k) * (k, m) = (m, n) 尺寸的矩阵乘法</span></span><br><span class="line"></span><br><span class="line"><span class="type">int</span> m = batch_size_ * from_seq_len_;</span><br><span class="line"><span class="type">int</span> k = head_num_ * size_per_head_;</span><br><span class="line"><span class="type">int</span> n = k;</span><br><span class="line"></span><br><span class="line"><span class="comment">// cublas的gemm是D = alpha * (A*B) + beta*C</span></span><br><span class="line"><span class="comment">// 我们的问题只需要D = A*B</span></span><br><span class="line"><span class="comment">// 所以alpha等于1，beta等于0</span></span><br><span class="line">DataType_ alpha = (DataType_)<span class="number">1.0f</span>, beta = (DataType_)<span class="number">0.0f</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span></span><br><span class="line">{</span><br><span class="line"> <span class="comment">// 输入的第一步是做input_tensor</span></span><br><span class="line"></span><br><span class="line"> <span class="comment">// 原始是row-major的数据，大小为：</span></span><br><span class="line"> <span class="comment">// Q: m*k</span></span><br><span class="line"> <span class="comment">// P: k*n</span></span><br><span class="line"> <span class="comment">// R: m*n</span></span><br><span class="line"> <span class="comment">// 但是cublasGemmEx需要的是列优先的数据。我们可以使用一个trick：</span></span><br><span class="line"> <span class="comment">// R = Q * P</span></span><br><span class="line"> <span class="comment">// R^T = P^T * Q^T</span></span><br><span class="line"> <span class="comment">// P^T的列优先数据就是和P的行优先数据在内存中是一样的, Q^T同理</span></span><br><span class="line"> <span class="comment">// 得到的列优先的R^T，其实和R使用行优先存储在内存中的数据是一样的</span></span><br><span class="line"> <span class="comment">// 所有求行优先存储的R(大小为[m,n])变成了[n, k]的P^T矩阵(内存数据不变)和</span></span><br><span class="line"> <span class="comment">// [k, m]的Q矩阵相乘得到的结果。</span></span><br><span class="line"> <span class="built_in">check_cuda_error</span>(<span class="built_in">cublasGemmEx</span>(param_.cublas_handle,</span><br><span class="line">                                CUBLAS_OP_N, CUBLAS_OP_N,</span><br><span class="line">                                n,m,k,</span><br><span class="line">                                &amp;alpha,</span><br><span class="line">                                param_.attr_kernel_Q,</span><br><span class="line">                                AType_,</span><br><span class="line">                                n,</span><br><span class="line">                                param_.from_tensor,</span><br><span class="line">                                BType_,</span><br><span class="line">                                k,</span><br><span class="line">                                &amp;beta,</span><br><span class="line">                                query_buf_,</span><br><span class="line">                                CType_,</span><br><span class="line">                                n,</span><br><span class="line">                                computeType_,</span><br><span class="line">                                <span class="built_in">static_cast</span>&lt;cublasGemmAlgo_t&gt;(cublasAlgo_[<span class="number">0</span>])));</span><br><span class="line"></span><br><span class="line"> <span class="built_in">check_cuda_error</span>(<span class="built_in">cublasGemmEx</span>(param_.cublas_handle,</span><br><span class="line">                                CUBLAS_OP_N, CUBLAS_OP_N,</span><br><span class="line">                                n,m,k,</span><br><span class="line">                                &amp;alpha,</span><br><span class="line">                                param_.attr_kernel_K,</span><br><span class="line">                                AType_,</span><br><span class="line">                                n,</span><br><span class="line">                                param_.from_tensor,</span><br><span class="line">                                BType_,</span><br><span class="line">                                k,</span><br><span class="line">                                &amp;beta,</span><br><span class="line">                                key_buf_,</span><br><span class="line">                                CType_,</span><br><span class="line">                                n,</span><br><span class="line">                                computeType_,</span><br><span class="line">                                <span class="built_in">static_cast</span>&lt;cublasGemmAlgo_t&gt;(cublasAlgo_[<span class="number">0</span>])));</span><br><span class="line"></span><br><span class="line"> <span class="built_in">check_cuda_error</span>(<span class="built_in">cublasGemmEx</span>(param_.cublas_handle,</span><br><span class="line">                                CUBLAS_OP_N, CUBLAS_OP_N,</span><br><span class="line">                                n,m,k,</span><br><span class="line">                                &amp;alpha,</span><br><span class="line">                                param_.attr_kernel_V,</span><br><span class="line">                                AType_,</span><br><span class="line">                                n,</span><br><span class="line">                                param_.from_tensor,</span><br><span class="line">                                BType_,</span><br><span class="line">                                k,</span><br><span class="line">                                &amp;beta,</span><br><span class="line">                                value_buf_,</span><br><span class="line">                                CType_,</span><br><span class="line">                                n,</span><br><span class="line">                                computeType_,</span><br><span class="line">                                <span class="built_in">static_cast</span>&lt;cublasGemmAlgo_t&gt;(cublasAlgo_[<span class="number">0</span>])));</span><br></pre></td></tr></table></figure></div>





<p>接着使用cuda kernel函数<code>add_QKV_bias</code>来实现偏置的加法以及将数据形状改变。</p>
<p><img lazyload="" src="/images/loading.svg" data-src="/images/FT_dataform.png" alt="image-20240124111632333"></p>
<p>数据保存方式如上所示。</p>
<p>行数就是m = batch_size * seq_len，列数为k = head_num * size_per_head。</p>
<p>我们使用3m（Q、K、V各m个block）个block（行），每个block使用k个线程（列）来处理。</p>
<p>这里word_per_block设置为1，每个block对于上面数据的一行来处理一个单词。</p>
<div class="highlight-container" data-rel="C++"><figure class="iseeu highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> m = batch_size * seq_len;</span><br><span class="line"><span class="type">int</span> k = head_num * size_per_head;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">add_QKV_bias</span><span class="params">(T*        Q,</span></span></span><br><span class="line"><span class="params"><span class="function">                             <span class="type">const</span> T*  bias_Q,</span></span></span><br><span class="line"><span class="params"><span class="function">                             T*        K,</span></span></span><br><span class="line"><span class="params"><span class="function">                             <span class="type">const</span> T*  bias_K,</span></span></span><br><span class="line"><span class="params"><span class="function">                             T*        V,</span></span></span><br><span class="line"><span class="params"><span class="function">                             <span class="type">const</span> T*  bias_V,</span></span></span><br><span class="line"><span class="params"><span class="function">                             T*        q_buf_,</span></span></span><br><span class="line"><span class="params"><span class="function">                             T*        k_buf_,</span></span></span><br><span class="line"><span class="params"><span class="function">                             T*        v_buf_,</span></span></span><br><span class="line"><span class="params"><span class="function">                             <span class="type">const</span> <span class="type">int</span> batch_size,</span></span></span><br><span class="line"><span class="params"><span class="function">                             <span class="type">const</span> <span class="type">int</span> seq_len,</span></span></span><br><span class="line"><span class="params"><span class="function">                             <span class="type">const</span> <span class="type">int</span> head_num,</span></span></span><br><span class="line"><span class="params"><span class="function">                             <span class="type">const</span> <span class="type">int</span> size_per_head,</span></span></span><br><span class="line"><span class="params"><span class="function">                             <span class="type">const</span> <span class="type">int</span> word_per_block)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line"></span><br><span class="line">    T*       data_ptr;</span><br><span class="line">    T*       buf_ptr;</span><br><span class="line">    <span class="type">const</span> T* bias_ptr;</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> m = batch_size * seq_len;</span><br><span class="line">    <span class="type">int</span> n = head_num * size_per_head;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 我们抛了3m个block</span></span><br><span class="line">    <span class="comment">// block [0...m-1]  : 处理Q</span></span><br><span class="line">    <span class="comment">// block [m...2m-1] : 处理K</span></span><br><span class="line">    <span class="comment">// block [2m...3m-1]: 处理V</span></span><br><span class="line">    <span class="type">int</span> qkv_id = blockIdx.x * word_per_block / m;</span><br><span class="line">    <span class="comment">// word_per_block = 1 代表每个block处理一个单词</span></span><br><span class="line">    <span class="comment">// (blockIdx.x * word_per_block / m)得到结果0、1、2分别代表当前处理的数据是q、k、v</span></span><br><span class="line">    <span class="comment">// 每个数据的长度是n</span></span><br><span class="line">    <span class="comment">// 所以row_offset能计算出第t行的数据相对于数据首地址的偏移  row_offset得到当前是q、k、v中的第几行 </span></span><br><span class="line">    <span class="type">int</span> row_offset = (blockIdx.x * word_per_block % m) * n;</span><br><span class="line">    <span class="comment">// 假设qkv_id = 0、row_offset = 1 代表当前数据属于q的第一个单词 </span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 处理Q</span></span><br><span class="line">    <span class="keyword">if</span> (qkv_id == <span class="number">0</span>)</span><br><span class="line">    {</span><br><span class="line">        data_ptr = Q + row_offset; <span class="comment">//Q + row_offset 为当前block第一个数据的地址 </span></span><br><span class="line">        buf_ptr  = q_buf_; <span class="comment">// 用来存储结果</span></span><br><span class="line">        bias_ptr = bias_Q; <span class="comment">// 的得到偏置参数</span></span><br><span class="line">    }</span><br><span class="line">    <span class="comment">// 处理K</span></span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span> (qkv_id == <span class="number">1</span>)</span><br><span class="line">    {</span><br><span class="line">        data_ptr = K + row_offset; </span><br><span class="line">        buf_ptr  = k_buf_;</span><br><span class="line">        bias_ptr = bias_K;</span><br><span class="line">    }</span><br><span class="line">    <span class="comment">// 处理V</span></span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    {</span><br><span class="line">        data_ptr = V + row_offset;</span><br><span class="line">        buf_ptr  = v_buf_;</span><br><span class="line">        bias_ptr = bias_V;</span><br><span class="line">    }</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Q的数据排列</span></span><br><span class="line">    <span class="comment">// batch0 word0          : (head0 0, head0 1,..., head0 size_per_head-1)(head1 0, )...</span></span><br><span class="line">    <span class="comment">// batch0 word1          :</span></span><br><span class="line">    <span class="comment">// batch0 word2          :</span></span><br><span class="line">    <span class="comment">// ...  :</span></span><br><span class="line">    <span class="comment">// batch0 word(seq_len-1):</span></span><br><span class="line">    <span class="comment">// block对于行 一行就是一个block</span></span><br><span class="line">    <span class="comment">// total threads in one block is head_num * size_per_head</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">// word_per_block = 1 ,  Q、K、V分别有m个block， blockIdx.x % m得到Q、K、V三大块中具体第几个block</span></span><br><span class="line">    <span class="comment">// 每一个bacth有seq_len个block， 所以上面的具体第几个block再 除以seq_len 得到第几个batch也就是batch_id</span></span><br><span class="line">    <span class="type">int</span> batch_id      = (blockIdx.x * word_per_block % m) / seq_len;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 每一个block中有k =  head_num * size_per_head 个线程</span></span><br><span class="line">    <span class="type">int</span> head_id       = threadIdx.x / size_per_head;</span><br><span class="line">    <span class="type">int</span> id_in_head    = threadIdx.x % size_per_head;<span class="comment">//得到当前数据在一个size_per_head长度中的位置</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">// 得到第几个单词</span></span><br><span class="line">    <span class="type">int</span> word_start_id = (blockIdx.x * word_per_block) % seq_len;</span><br><span class="line"></span><br><span class="line">    T bias = __ldg(&amp;bias_ptr[threadIdx.x]);<span class="comment">//__ldg用于从全局内存加载数据 提高访问性能</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = word_start_id; i &lt; word_start_id + word_per_block; ++i)</span><br><span class="line">    {   </span><br><span class="line">        <span class="comment">// 将原本数据结果加上偏置</span></span><br><span class="line">        T tmp = data_ptr[threadIdx.x] + bias;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 将数据存储方式改变为 </span></span><br><span class="line">        <span class="comment">// batch0    :  head0 : word0  (head0 0, head0 0,..., head0 size_per_head-1)</span></span><br><span class="line">        <span class="comment">//                      word1  (head0 0, head1 0,..., head0 size_per_head-1)</span></span><br><span class="line">        <span class="comment">//                           ...</span></span><br><span class="line">        <span class="comment">//              head1 : word0  (head1 0, head1 1,..., head1 size_per_head-1)</span></span><br><span class="line">        <span class="comment">//                      word1  (head1 0, head1 1,..., head1 size_per_head-1)</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">// batch1    : head0  : word0  (head0 0, head0 1,..., head0 size_per_head-1)</span></span><br><span class="line">        <span class="comment">//                      word1  (head0 0, head0 1,..., head0 size_per_head-1)</span></span><br><span class="line">        <span class="comment">//</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 一个batch 有seq_len * head_num * size_per_head个数据</span></span><br><span class="line">        <span class="comment">// 一个head 中有seq_len * size_per_head 个数据</span></span><br><span class="line">        <span class="comment">// 每个word 中有size_per_head 个数据</span></span><br><span class="line">        <span class="comment">// id_in_head 则为每个单词维度中具体第几个数据</span></span><br><span class="line">        </span><br><span class="line">        <span class="type">int</span> target_id = batch_id * (seq_len * head_num * size_per_head) +</span><br><span class="line">                        head_id * seq_len * size_per_head + i * size_per_head + id_in_head;</span><br><span class="line">        <span class="comment">//将数据存在buf_ptr</span></span><br><span class="line">        buf_ptr[target_id] = tmp;</span><br><span class="line">        </span><br><span class="line">        data_ptr += n;</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></table></figure></div>



<p>改变存储方式后的数据形状如下所示：</p>
<p><img lazyload="" src="/images/loading.svg" data-src="/images/FT_dataform2.png" alt="fastertransformer2"></p>
<h3 id="二、Matmul"><a href="#二、Matmul" class="headerlink" title="二、Matmul"></a>二、Matmul</h3><p>这一层调用cubalsGemmStridedBatchedEx函数来实现。</p>
<p>在计算<mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -0.439ex;" xmlns="http://www.w3.org/2000/svg" width="7.369ex" height="2.456ex" role="img" focusable="false" viewBox="0 -891.7 3257.2 1085.7"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D444" d="M399 -80Q399 -47 400 -30T402 -11V-7L387 -11Q341 -22 303 -22Q208 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435Q740 255 592 107Q529 47 461 16L444 8V3Q444 2 449 -24T470 -66T516 -82Q551 -82 583 -60T625 -3Q631 11 638 11Q647 11 649 2Q649 -6 639 -34T611 -100T557 -165T481 -194Q399 -194 399 -87V-80ZM636 468Q636 523 621 564T580 625T530 655T477 665Q429 665 379 640Q277 591 215 464T153 216Q153 110 207 59Q231 38 236 38V46Q236 86 269 120T347 155Q372 155 390 144T417 114T429 82T435 55L448 64Q512 108 557 185T619 334T636 468ZM314 18Q362 18 404 39L403 49Q399 104 366 115Q354 117 347 117Q344 117 341 117T337 118Q317 118 296 98T274 52Q274 18 314 18Z"></path></g><g data-mml-node="mo" transform="translate(1013.2,0)"><path data-c="2217" d="M229 286Q216 420 216 436Q216 454 240 464Q241 464 245 464T251 465Q263 464 273 456T283 436Q283 419 277 356T270 286L328 328Q384 369 389 372T399 375Q412 375 423 365T435 338Q435 325 425 315Q420 312 357 282T289 250L355 219L425 184Q434 175 434 161Q434 146 425 136T401 125Q393 125 383 131T328 171L270 213Q283 79 283 63Q283 53 276 44T250 35Q231 35 224 44T216 63Q216 80 222 143T229 213L171 171Q115 130 110 127Q106 124 100 124Q87 124 76 134T64 161Q64 166 64 169T67 175T72 181T81 188T94 195T113 204T138 215T170 230T210 250L74 315Q65 324 65 338Q65 353 74 363T98 374Q106 374 116 368T171 328L229 286Z"></path></g><g data-mml-node="msup" transform="translate(1735.4,0)"><g data-mml-node="mi"><path data-c="1D43E" d="M285 628Q285 635 228 637Q205 637 198 638T191 647Q191 649 193 661Q199 681 203 682Q205 683 214 683H219Q260 681 355 681Q389 681 418 681T463 682T483 682Q500 682 500 674Q500 669 497 660Q496 658 496 654T495 648T493 644T490 641T486 639T479 638T470 637T456 637Q416 636 405 634T387 623L306 305Q307 305 490 449T678 597Q692 611 692 620Q692 635 667 637Q651 637 651 648Q651 650 654 662T659 677Q662 682 676 682Q680 682 711 681T791 680Q814 680 839 681T869 682Q889 682 889 672Q889 650 881 642Q878 637 862 637Q787 632 726 586Q710 576 656 534T556 455L509 418L518 396Q527 374 546 329T581 244Q656 67 661 61Q663 59 666 57Q680 47 717 46H738Q744 38 744 37T741 19Q737 6 731 0H720Q680 3 625 3Q503 3 488 0H478Q472 6 472 9T474 27Q478 40 480 43T491 46H494Q544 46 544 71Q544 75 517 141T485 216L427 354L359 301L291 248L268 155Q245 63 245 58Q245 51 253 49T303 46H334Q340 37 340 35Q340 19 333 5Q328 0 317 0Q314 0 280 1T180 2Q118 2 85 2T49 1Q31 1 31 11Q31 13 34 25Q38 41 42 43T65 46Q92 46 125 49Q139 52 144 61Q147 65 216 339T285 628Z"></path></g><g data-mml-node="mi" transform="translate(974,413) scale(0.707)"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"></path></g></g></g></g></svg></mjx-container>时，只能是每一个head对应的q、k相乘。</p>
<p>使用cubalsGemmStridedBatchedEx函数来实现A0 * B0、A1 * B1、……</p>
<p>A的数据按行来拼接，这也是为什么上一步需要将数据形状改变。</p>
<p><img lazyload="" src="/images/loading.svg" data-src="/images/FT_strideGemm.png" alt="strideGemm"></p>
<div class="highlight-container" data-rel="C++"><figure class="iseeu highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">check_cuda_error</span>(<span class="built_in">cublasGemmStridedBatchedEx</span>(cublas_handle,</span><br><span class="line">    CUBLAS_OP_T, CUBLAS_OP_N,</span><br><span class="line">    seq_len, seq_len, size_per_head,</span><br><span class="line">    &amp;alpha,</span><br><span class="line">    k_buf_, AType_, size_per_head, seq_len * size_per_head, <span class="comment">// 这里要注意lda的参数 因为使用了CUBLAS_OP_T这里为矩阵的列</span></span><br><span class="line">    q_buf_, BType_, size_per_head, seq_len * size_per_head,</span><br><span class="line">    &amp;beta,</span><br><span class="line">    qk_buf_, CType_, seq_len, seq_len * seq_len,</span><br><span class="line">    batch_size * head_num,</span><br><span class="line">    computeType_,</span><br><span class="line">    <span class="built_in">static_cast</span>&lt;cublasGemmAlgo_t&gt;(cublasAlgo_[<span class="number">1</span>])));</span><br></pre></td></tr></table></figure></div>

<p>这里m、n、k参数都是对应做乘法的小矩阵的维度，而不是整个大矩阵的维度。</p>
<p>这一步后得到的数据大小为（batch_size, head_num, seq_len * seq_len）。</p>
<h3 id="三、softmax"><a href="#三、softmax" class="headerlink" title="三、softmax"></a>三、softmax</h3><p>softmax对权值进行归一化，这里主要涉及到两个规约，一个是max_reduce找到最大值，还有一个sum_reduce计算所有值的和。</p>
<p>这里根据数据的大小来选择两个softmax函数<strong>softmax_kernel</strong>和<strong>softmax_kernel_V2</strong> 。</p>
<div class="highlight-container" data-rel="C++"><figure class="iseeu highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @brief softmax(Q*K^T/sqrt(D))</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * 该函数单独计算softmax，数据没有重排。</span></span><br><span class="line"><span class="comment"> * 上一步输出的矩阵尺寸为 (batch_size * head_num) * (seq_len * seq_len)</span></span><br><span class="line"><span class="comment"> * 使用batch_size * head_num个block，每个block处理seq_len * seq_len的矩阵。</span></span><br><span class="line"><span class="comment"> * 一个block的线程数是大于等于seq_len的，每次处理的时候以行为单位进行处理，每次处理</span></span><br><span class="line"><span class="comment"> * 一行，seq_len次循环处理完所有的数据</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * @tparam T            fp16 or float</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * @param qk_buf_       输入的Q*K^T [batch_size, head_num, seq_len*seq_len]</span></span><br><span class="line"><span class="comment"> * @param attr_mask     mask数据，大小为[batch_size, seq_len*seq_len]</span></span><br><span class="line"><span class="comment"> * @param batch_size    sizeo f batch</span></span><br><span class="line"><span class="comment"> * @param head_num      number of head</span></span><br><span class="line"><span class="comment"> * @param seq_len       sequence lenght in a batch</span></span><br><span class="line"><span class="comment"> * @param scaler        1.0 / sqrt(size_per_head)</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * @return void</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">softmax_kernel</span><span class="params">(T*        qk_buf_,</span></span></span><br><span class="line"><span class="params"><span class="function">                               <span class="type">const</span> T*  attr_mask,</span></span></span><br><span class="line"><span class="params"><span class="function">                               <span class="type">const</span> <span class="type">int</span> batch_size,</span></span></span><br><span class="line"><span class="params"><span class="function">                               <span class="type">const</span> <span class="type">int</span> head_num,</span></span></span><br><span class="line"><span class="params"><span class="function">                               <span class="type">const</span> <span class="type">int</span> seq_len,</span></span></span><br><span class="line"><span class="params"><span class="function">                               <span class="type">const</span> T   scaler)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> batch_id    = blockIdx.x / head_num;</span><br><span class="line">    <span class="type">int</span> qk_offset   = blockIdx.x * seq_len * seq_len;</span><br><span class="line">    <span class="type">int</span> mask_offset = batch_id * seq_len * seq_len;</span><br><span class="line"></span><br><span class="line">    __shared__ <span class="type">float</span> s_sum, s_max;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; seq_len; ++i)</span><br><span class="line">    {</span><br><span class="line">        <span class="type">float</span> qk       = threadIdx.x &lt; seq_len ? (<span class="type">float</span>)qk_buf_[threadIdx.x + qk_offset] : <span class="number">0.0f</span>;</span><br><span class="line">        <span class="type">float</span> mask_val = threadIdx.x &lt; seq_len ? (<span class="type">float</span>)attr_mask[threadIdx.x + mask_offset] : <span class="number">0.0f</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// mask为1的位置值为0，否则为-10000.0f(模拟-inf)</span></span><br><span class="line">        mask_val = (<span class="number">1.0f</span> - mask_val) * <span class="number">-10000.0f</span>;</span><br><span class="line"></span><br><span class="line">        <span class="type">float</span> tmp = threadIdx.x &lt; seq_len ? (<span class="type">float</span>)(qk * (<span class="type">float</span>)scaler + mask_val) : <span class="number">-1e20</span>f;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 计算这一行的最大值 直接调用库函数</span></span><br><span class="line">        <span class="type">float</span> max_val = <span class="built_in">blockReduceMax</span>&lt;<span class="type">float</span>&gt;(tmp);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 把最大值写入到shared memory，让所有线程都可见</span></span><br><span class="line">        <span class="keyword">if</span> (threadIdx.x == <span class="number">0</span>)</span><br><span class="line">            s_max = max_val;</span><br><span class="line">        __syncthreads();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// exp(x - xmax)</span></span><br><span class="line">        qk = threadIdx.x &lt; seq_len ? __expf(tmp - s_max) : <span class="number">0.0f</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// sum exp(x - xmax)</span></span><br><span class="line">        <span class="type">float</span> sum_val = <span class="built_in">blockReduceSum</span>&lt;<span class="type">float</span>&gt;(qk);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 把sum写入到shared memory，让所有线程可见</span></span><br><span class="line">        <span class="keyword">if</span> (threadIdx.x == <span class="number">0</span>)</span><br><span class="line">        {</span><br><span class="line">            s_sum = sum_val + <span class="number">1e-6</span>f;</span><br><span class="line">        }</span><br><span class="line">        __syncthreads();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 把softmax的值写出</span></span><br><span class="line">        <span class="keyword">if</span> (threadIdx.x &lt; seq_len)</span><br><span class="line">            qk_buf_[threadIdx.x + qk_offset] = (T)(qk / s_sum);</span><br><span class="line"></span><br><span class="line">        qk_offset += seq_len;</span><br><span class="line">        mask_offset += seq_len;</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></table></figure></div>

<p>softmax_kernel_V2</p>
<div class="highlight-container" data-rel="C++"><figure class="iseeu highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @brief softmax(Q*K^T/sqrt(D))</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * 该函数单独计算softmax，数据没有重排。</span></span><br><span class="line"><span class="comment"> * 上一步输出的矩阵尺寸为 (batch_size * head_num) * (seq_len * seq_len)</span></span><br><span class="line"><span class="comment"> * 使用batch_size * head_num * seq_len个block，每个block处理seq_len的数据，也就是一行数据。</span></span><br><span class="line"><span class="comment"> * 一个block的线程数是大于等于seq_len的，每次处理的时候以行为单位进行处理，每次处理</span></span><br><span class="line"><span class="comment"> * 一行，seq_len次循环处理完所有的数据</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * @tparam T            fp16 or float</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * @param qk_buf_       输入的Q*K^T [batch_size, head_num, seq_len*seq_len]</span></span><br><span class="line"><span class="comment"> * @param attr_mask     mask数据，大小为[batch_size, seq_len*seq_len]</span></span><br><span class="line"><span class="comment"> * @param batch_size    sizeo f batch</span></span><br><span class="line"><span class="comment"> * @param head_num      number of head</span></span><br><span class="line"><span class="comment"> * @param seq_len       sequence lenght in a batch</span></span><br><span class="line"><span class="comment"> * @param scaler        1.0 / sqrt(size_per_head)</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * @return</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">softmax_kernel_v2</span><span class="params">(T*          qk_buf_,</span></span></span><br><span class="line"><span class="params"><span class="function">                                  <span class="type">const</span> T*    attr_mask,</span></span></span><br><span class="line"><span class="params"><span class="function">                                  <span class="type">const</span> <span class="type">int</span>   batch_size,</span></span></span><br><span class="line"><span class="params"><span class="function">                                  <span class="type">const</span> <span class="type">int</span>   head_num,</span></span></span><br><span class="line"><span class="params"><span class="function">                                  <span class="type">const</span> <span class="type">int</span>   seq_len,</span></span></span><br><span class="line"><span class="params"><span class="function">                                  <span class="type">const</span> <span class="type">float</span> scaler)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="type">int</span> batch_id    = blockIdx.x / head_num / seq_len;</span><br><span class="line">    <span class="type">int</span> seq_id      = blockIdx.x % seq_len;</span><br><span class="line">    <span class="type">int</span> qk_offset   = blockIdx.x * seq_len;</span><br><span class="line">    <span class="type">int</span> mask_offset = batch_id * seq_len * seq_len + seq_id * seq_len;</span><br><span class="line"></span><br><span class="line">    __shared__ <span class="type">float</span> s_sum, s_max;</span><br><span class="line"></span><br><span class="line">    <span class="type">float</span> qk       = threadIdx.x &lt; seq_len ? (<span class="type">float</span>)qk_buf_[threadIdx.x + qk_offset] : <span class="number">0.0f</span>;</span><br><span class="line">    <span class="type">float</span> mask_val = threadIdx.x &lt; seq_len ? (<span class="type">float</span>)attr_mask[threadIdx.x + mask_offset] : <span class="number">0.0f</span>;</span><br><span class="line"></span><br><span class="line">    mask_val = (<span class="number">1.0f</span> - mask_val) * <span class="number">-10000.0f</span>;</span><br><span class="line"></span><br><span class="line">    <span class="type">float</span> tmp     = threadIdx.x &lt; seq_len ? (<span class="type">float</span>)(qk * (<span class="type">float</span>)scaler + mask_val) : <span class="number">-1e20</span>f;</span><br><span class="line">    <span class="type">float</span> max_val = <span class="built_in">blockReduceMax</span>&lt;<span class="type">float</span>&gt;(tmp);</span><br><span class="line">    <span class="keyword">if</span> (threadIdx.x == <span class="number">0</span>)</span><br><span class="line">        s_max = max_val;</span><br><span class="line">    __syncthreads();</span><br><span class="line"></span><br><span class="line">    <span class="type">float</span> qk_tmp  = threadIdx.x &lt; seq_len ? __expf((<span class="type">float</span>)(tmp - s_max)) : <span class="number">0.0f</span>;</span><br><span class="line">    <span class="type">float</span> sum_val = <span class="built_in">blockReduceSum</span>&lt;<span class="type">float</span>&gt;(qk_tmp);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (threadIdx.x == <span class="number">0</span>)</span><br><span class="line">    {</span><br><span class="line">        s_sum = sum_val + <span class="number">1e-6</span>f;</span><br><span class="line">    }</span><br><span class="line">    __syncthreads();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (threadIdx.x &lt; seq_len)</span><br><span class="line">        qk_buf_[threadIdx.x + qk_offset] = (T)(qk_tmp / s_sum);</span><br><span class="line">}</span><br></pre></td></tr></table></figure></div>



<h3 id="四、Matmul-计算-V"><a href="#四、Matmul-计算-V" class="headerlink" title="四、Matmul 计算 *V"></a>四、Matmul 计算 *V</h3><p>这一步也是同样使用cubalsGemmStridedBatchedEx函数来进行计算。</p>
<div class="highlight-container" data-rel="C++"><figure class="iseeu highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">check_cuda_error</span>(<span class="built_in">cublasGemmStridedBatchedEx</span>(cublas_handle,</span><br><span class="line">                                                CUBLAS_OP_N,</span><br><span class="line">                                                CUBLAS_OP_N,</span><br><span class="line">                                                size_per_head,</span><br><span class="line">                                                seq_len,</span><br><span class="line">                                                seq_len,</span><br><span class="line">                                                &amp;alpha,</span><br><span class="line">                                                v_buf_,</span><br><span class="line">                                                AType_,</span><br><span class="line">                                                size_per_head,</span><br><span class="line">                                                seq_len * size_per_head,</span><br><span class="line">                                                qk_buf_,</span><br><span class="line">                                                BType_,</span><br><span class="line">                                                seq_len,</span><br><span class="line">                                                seq_len * seq_len,</span><br><span class="line">                                                &amp;beta,</span><br><span class="line">                                                transpose_dst_,</span><br><span class="line">                                                CType_,</span><br><span class="line">                                                size_per_head,</span><br><span class="line">                                                seq_len * size_per_head,</span><br><span class="line">                                                batch_size * head_num,</span><br><span class="line">                                                computeType_,</span><br><span class="line">                                                <span class="built_in">static_cast</span>&lt;cublasGemmAlgo_t&gt;(cublasAlgo_[<span class="number">2</span>])));</span><br><span class="line"></span><br></pre></td></tr></table></figure></div>



<h3 id="五、输出线性层"><a href="#五、输出线性层" class="headerlink" title="五、输出线性层"></a>五、输出线性层</h3><h4 id="transpose"><a href="#transpose" class="headerlink" title="transpose"></a>transpose</h4><p>首先使用transpose把数据排布从[batch_size, head_num, seq_length, size_per_head]转换成[batch_size. seq_length, head_num, size_per_head]。</p>
<p><strong>因为我们需要转换成单个head的数据方便使用cublasGemmStridedBatchedEx，而输入数据和后续处理的数据都是采用的[batch_size. seq_length, head_num, size_per_head]格式，所以在调用完batchgemm函数之后需要把格式转换回去。</strong></p>
<div class="highlight-container" data-rel="C++"><figure class="iseeu highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 把[batch_size, head_num, seq_length, size_per_head]转换成</span></span><br><span class="line"><span class="comment"> *   [batch_size. seq_length, head_num, size_per_head]</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * 使用grid.x = batch_size * head_num * seq_len;</span></span><br><span class="line"><span class="comment"> *    block.x = size_per_head;</span></span><br><span class="line"><span class="comment"> * 也就是每个block处理输入数据的一行</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * @note 这个函数应该对GPU的利用率比较低，因为seq_len不会特别大也未必是32的整数倍，考虑优化方法</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * @param[in]  src        softmax(Q*K^T/sqrt(D))*V的结果矩阵</span></span><br><span class="line"><span class="comment"> * @param[out] dst        输出转换后的矩阵</span></span><br><span class="line"><span class="comment"> * @param[in]  batch_size</span></span><br><span class="line"><span class="comment"> * @param[in]  seq_len</span></span><br><span class="line"><span class="comment"> * @param[in]  head_num</span></span><br><span class="line"><span class="comment"> * @param[in]  size_per_head</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">transpose</span><span class="params">(T*        src,</span></span></span><br><span class="line"><span class="params"><span class="function">                          T*        dst,</span></span></span><br><span class="line"><span class="params"><span class="function">                          <span class="type">const</span> <span class="type">int</span> batch_size,</span></span></span><br><span class="line"><span class="params"><span class="function">                          <span class="type">const</span> <span class="type">int</span> seq_len,</span></span></span><br><span class="line"><span class="params"><span class="function">                          <span class="type">const</span> <span class="type">int</span> head_num,</span></span></span><br><span class="line"><span class="params"><span class="function">                          <span class="type">const</span> <span class="type">int</span> size_per_head)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="type">int</span> batch_id                               = blockIdx.x / (head_num * seq_len);</span><br><span class="line">    <span class="type">int</span> seq_id                                 = blockIdx.x % seq_len;</span><br><span class="line">    <span class="type">int</span> head_id                                = (blockIdx.x % (head_num * seq_len)) / seq_len;</span><br><span class="line">    dst[batch_id * (head_num * seq_len * size_per_head) + seq_id * head_num * size_per_head +</span><br><span class="line">        head_id * size_per_head + threadIdx.x] = src[blockIdx.x * size_per_head + threadIdx.x];</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span>&lt;&gt;</span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">transpose</span><span class="params">(__half*   src,</span></span></span><br><span class="line"><span class="params"><span class="function">                          __half*   dst,</span></span></span><br><span class="line"><span class="params"><span class="function">                          <span class="type">const</span> <span class="type">int</span> batch_size,</span></span></span><br><span class="line"><span class="params"><span class="function">                          <span class="type">const</span> <span class="type">int</span> seq_len,</span></span></span><br><span class="line"><span class="params"><span class="function">                          <span class="type">const</span> <span class="type">int</span> head_num,</span></span></span><br><span class="line"><span class="params"><span class="function">                          <span class="type">const</span> <span class="type">int</span> size_per_head)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    <span class="type">int</span> tid = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> batch_id = tid / (head_num * seq_len * size_per_head);</span><br><span class="line">    <span class="type">int</span> head_id  = (tid % (head_num * seq_len * size_per_head)) / (seq_len * size_per_head);</span><br><span class="line">    <span class="type">int</span> seq_id   = (tid % (seq_len * size_per_head)) / size_per_head;</span><br><span class="line">    <span class="type">int</span> id       = tid % size_per_head;</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> target_id =</span><br><span class="line">        <span class="built_in">target_index</span>(batch_id, head_id, seq_id, id, batch_size, head_num, seq_len, size_per_head);</span><br><span class="line">    half2* src_ptr = (half2*)src;</span><br><span class="line">    half2* dst_ptr = (half2*)dst;</span><br><span class="line"></span><br><span class="line">    dst_ptr[target_id] = src_ptr[tid];</span><br><span class="line">}</span><br></pre></td></tr></table></figure></div>



<h4 id="linear"><a href="#linear" class="headerlink" title="linear"></a>linear</h4><p>这里使用cubalsGemmEx函数进行矩阵乘法，实现线程层，这里的bias依旧在下一个kernel中进行。</p>
<div class="highlight-container" data-rel="C++"><figure class="iseeu highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 输入矩阵尺寸: [batch_size, seq_len, head_num, size_per_head]</span></span><br><span class="line"><span class="comment">// 可以看做二维矩阵: I = [batch_size*seq_len, head_num*size_per_head]</span></span><br><span class="line"><span class="comment">// Linear层矩阵尺寸: P = [head_num*size_per_head, head_num*size_per_head]</span></span><br><span class="line"><span class="comment">// Linear层bias尺寸: B = [head_num]</span></span><br><span class="line"><span class="comment">// 这里计算 I * P</span></span><br><span class="line">DataType_ alpha = (DataType_)<span class="number">1.0f</span>;</span><br><span class="line">DataType_ beta  = (DataType_)<span class="number">0.0f</span>;</span><br><span class="line"><span class="type">int</span>       m     = batch_size_ * from_seq_len_;</span><br><span class="line"><span class="type">int</span>       k     = head_num_ * size_per_head_;</span><br><span class="line"><span class="type">int</span>       n     = k;</span><br><span class="line"></span><br><span class="line"><span class="built_in">check_cuda_error</span>(<span class="built_in">cublasGemmEx</span>(param_.cublas_handle,</span><br><span class="line">                              CUBLAS_OP_N,</span><br><span class="line">                              CUBLAS_OP_N,</span><br><span class="line">                              n,</span><br><span class="line">                              m,</span><br><span class="line">                              k,</span><br><span class="line">                              &amp;alpha,</span><br><span class="line">                              param_.attr_output_kernel,</span><br><span class="line">                              AType_,</span><br><span class="line">                              n,</span><br><span class="line">                              attr_out_buf_,</span><br><span class="line">                              BType_,</span><br><span class="line">                              k,</span><br><span class="line">                              &amp;beta,</span><br><span class="line">                              attr_matmul_buf_,</span><br><span class="line">                              CType_,</span><br><span class="line">                              n,</span><br><span class="line">                              computeType_,</span><br><span class="line">                              <span class="built_in">static_cast</span>&lt;cublasGemmAlgo_t&gt;(cublasAlgo_[<span class="number">0</span>])));</span><br></pre></td></tr></table></figure></div>

<p>输出为（batch_size, seq_len, head_num, size_per_head）</p>
<h4 id="add-bias-input-layernorm"><a href="#add-bias-input-layernorm" class="headerlink" title="add_bias_input_layernorm"></a>add_bias_input_layernorm</h4><p>这里的add_bias_input_layernorm主要实现三个步骤：</p>
<ol>
<li>加上linear的bias</li>
<li>加上一开始的input，实现残差连接</li>
<li>实现layernormal</li>
</ol>
<div class="highlight-container" data-rel="C++"><figure class="iseeu highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @brief                       grid_size = m, block_size = n</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * @tparam T </span></span><br><span class="line"><span class="comment"> * @param out                   [batch_size, sql_len, latent_dim]</span></span><br><span class="line"><span class="comment"> * @param input                 [batch_size, sql_len, latent_dim]</span></span><br><span class="line"><span class="comment"> * @param bias                  [latent_dim,]</span></span><br><span class="line"><span class="comment"> * @param gamma </span></span><br><span class="line"><span class="comment"> * @param beta </span></span><br><span class="line"><span class="comment"> * @param m                     batch_size * seq_len</span></span><br><span class="line"><span class="comment"> * @param n                     latent_dim</span></span><br><span class="line"><span class="comment"> * @return __global__ </span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function">__global__ </span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">add_bias_input_layernorm</span><span class="params">(T* out, <span class="type">const</span> T* input, <span class="type">const</span> T* bias, <span class="type">const</span> T* gamma, <span class="type">const</span> T* beta, <span class="type">int</span> m, <span class="type">int</span> n)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">  <span class="type">int</span> tid = threadIdx.x;</span><br><span class="line"></span><br><span class="line">  __shared__ <span class="type">float</span> s_mean;</span><br><span class="line">  __shared__ <span class="type">float</span> s_variance;</span><br><span class="line">  <span class="type">float</span> mean =  <span class="number">0.0f</span>;</span><br><span class="line">  <span class="type">float</span> variance = <span class="number">0.0f</span>;</span><br><span class="line"></span><br><span class="line">  <span class="type">float</span> local_out = <span class="number">0.0f</span>;</span><br><span class="line">  <span class="comment">// add，一个block处理一行</span></span><br><span class="line">  <span class="keyword">for</span>(<span class="type">int</span> i = tid; i &lt; n; i += blockDim.x)</span><br><span class="line">    local_out += (<span class="type">float</span>)(out[blockIdx.x * n + i] + input[blockIdx.x * n + i] + __ldg(&amp;bias[i]));</span><br><span class="line">  <span class="comment">// mean_i = sum(x_i[j] for j in range(k)) / k</span></span><br><span class="line">  mean = <span class="built_in">blockReduceSum</span>&lt;<span class="type">float</span>&gt;(local_out);</span><br><span class="line">  <span class="keyword">if</span>(threadIdx.x == <span class="number">0</span>)</span><br><span class="line">    s_mean = mean / n;</span><br><span class="line">  __syncthreads();</span><br><span class="line">  <span class="comment">// var_i = sum((x_i[j] - mean_i) ** 2 for j in range(k)) / k + epsilon</span></span><br><span class="line">  variance = <span class="built_in">blockReduceSum</span>&lt;<span class="type">float</span>&gt;((local_out - s_mean) * (local_out - s_mean));</span><br><span class="line">  <span class="keyword">if</span>(threadIdx.x == <span class="number">0</span>)</span><br><span class="line">    s_variance = variance / n + <span class="number">1e-6</span>f;</span><br><span class="line">  __syncthreads();</span><br><span class="line">  <span class="comment">// x_i_normalized = (x_i - mean_i) / sqrt(var_i)</span></span><br><span class="line">  <span class="comment">// output_i = x_i_normalized * gamma + beta</span></span><br><span class="line">  <span class="keyword">for</span>(<span class="type">int</span> i = tid; i &lt; n; i += blockDim.x)</span><br><span class="line">    out[blockIdx.x * n + i] = </span><br><span class="line">	    (T)(((local_out - s_mean) * <span class="built_in">rsqrtf</span>(s_variance)) * (<span class="type">float</span>)(__ldg(&amp;gamma[i])) + (<span class="type">float</span>)(__ldg(&amp;beta[i])));</span><br><span class="line">}</span><br></pre></td></tr></table></figure></div>





<h2 id="FFN"><a href="#FFN" class="headerlink" title="FFN"></a>FFN</h2><p>经过上面的多头注意力和残差连接后，下面是一块逐位前馈网络（feed forward network）。</p>
<p>具体的展开结构就是，一层线性变换+GELU激活函数+一层线性变换+layernormal+残差连接。</p>
<p><img lazyload="" src="/images/loading.svg" data-src="/images/FT_FFN.png" alt="FFN"></p>
<h3 id="一、第一层线性变换"><a href="#一、第一层线性变换" class="headerlink" title="一、第一层线性变换"></a>一、第一层线性变换</h3><p>这里在fastertransformer中将隐藏维度扩大了四倍。对应乘上的参数矩阵大小就是（embedding_size, 4*embedding_size）</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cublasGemmEx(cublas_handle, CUBLAS_OP_N, CUBLAS_OP_N, </span><br><span class="line">       4*embedding_size, batch_size * seq_len, embedding_size,</span><br><span class="line">       &amp;alpha, </span><br><span class="line">       ffn_weight1, CUDA_R_32F, 4*embedding_size,</span><br><span class="line">       self_att_output, CUDA_R_32F, embedding_size,</span><br><span class="line">       &amp;beta,</span><br><span class="line">       ffn_multi1_out, CUDA_R_32F, 4*embedding_size,</span><br><span class="line">       CUDA_R_32F, CUBLAS_GEMM_DEFAULT);</span><br></pre></td></tr></table></figure></div>



<h3 id="二、add-bias-act"><a href="#二、add-bias-act" class="headerlink" title="二、add_bias_act"></a>二、add_bias_act</h3><p>计算激活函数时，将前一层的加bias操作放在一起进行。</p>
<div class="highlight-container" data-rel="C++"><figure class="iseeu highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * @brief 计算GELU(I + bias)</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * @param out   输出数据指针，大小为[batch_size*seq_len, 4*head_num*size_per_head]</span></span><br><span class="line"><span class="comment"> * @param bias  bias数据，大小为[1, 4*head_num*size_per_head]</span></span><br><span class="line"><span class="comment"> * @param m     值为batch_size*seq_len</span></span><br><span class="line"><span class="comment"> * @param n     值为4*head_num*size_per_head</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * gridDim.x = batch_size*seq_l / 4 这里在while循环里处理4行数据</span></span><br><span class="line"><span class="comment"> * blockDim.x = head_num*size_per_head 这里在一个block中计算一行 防止4*head_num*size_per_head过大  所以使用head_num*size_per_head个线程再循环4次计算</span></span><br><span class="line"><span class="comment"> * @return void</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function">__device__ T <span class="title">gelu</span><span class="params">(T x)</span> </span>{</span><br><span class="line">    T cdf = <span class="number">0.5</span> * (<span class="number">1.0</span> + <span class="built_in">tanh</span>(<span class="built_in">sqrt</span>(<span class="number">2.0</span> / M_PI) * (x + <span class="number">0.044715</span> * <span class="built_in">pow</span>(x, <span class="number">3</span>))));</span><br><span class="line">    <span class="keyword">return</span> x * cdf;</span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">add_bias_act</span><span class="params">(T* out, <span class="type">const</span> T* bias, <span class="type">int</span> m, <span class="type">int</span> n)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">    T val, reg_bias;</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> row_id = blockIdx.x;</span><br><span class="line">    <span class="type">int</span> ite    = n / blockDim.x; <span class="comment">//这里的n是4*head_num*size_per_head blockDim.x = head_num*size_per_head</span></span><br><span class="line">    <span class="type">int</span> tid    = threadIdx.x;</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; ite; ++i)</span><br><span class="line">    {</span><br><span class="line">        reg_bias = __ldg(&amp;bias[i * blockDim.x + tid]);</span><br><span class="line">        row_id   = blockIdx.x;<span class="comment">//重新定位到当前block处理的第一行</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">// gridDim.x = m/4 所以这里一共循环4次 第一个block处理第0行 第0+m/4行 第0+2*m/4行 第0+3*m/4行 </span></span><br><span class="line">        <span class="keyword">while</span> (row_id &lt; m)</span><br><span class="line">        {   <span class="comment">//加上bias</span></span><br><span class="line">            val = out[tid + i * blockDim.x + row_id * n] + reg_bias;</span><br><span class="line">            <span class="comment">// 计算激活函数</span></span><br><span class="line">            out[tid + i * blockDim.x + row_id * n] = <span class="built_in">gelu</span>&lt;T&gt;(val);</span><br><span class="line">            row_id += gridDim.x;<span class="comment">// 加上m/4</span></span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></table></figure></div>



<h3 id="三、第二层线性变换"><a href="#三、第二层线性变换" class="headerlink" title="三、第二层线性变换"></a>三、第二层线性变换</h3><p>这里把维度又变回embedding_size。</p>
<div class="highlight-container" data-rel="C++"><figure class="iseeu highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cublasGemmEx</span>(cublas_handle, CUBLAS_OP_N, CUBLAS_OP_N, </span><br><span class="line">        embedding_size, batch_size * seq_len, <span class="number">4</span>*embedding_size,</span><br><span class="line">        &amp;alpha, </span><br><span class="line">        ffn_weight2, CUDA_R_32F, embedding_size,</span><br><span class="line">        ffn_multi1_out, CUDA_R_32F, <span class="number">4</span>*embedding_size,</span><br><span class="line">        &amp;beta,</span><br><span class="line">        ffn_multi2_out, CUDA_R_32F, embedding_size,</span><br><span class="line">        CUDA_R_32F, CUBLAS_GEMM_DEFAULT);</span><br></pre></td></tr></table></figure></div>



<h3 id="四、add-bias-input-layernorm"><a href="#四、add-bias-input-layernorm" class="headerlink" title="四、add_bias_input_layernorm"></a>四、add_bias_input_layernorm</h3><p>这里和上面attention块中使用的是同一个函数。</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">add_bias_input_layernorm&lt;&lt;&lt;grid_add_bias, block_add_bias&gt;&gt;&gt;(ffn_multi2_out, self_att_output,ffn_bias2, </span><br><span class="line">                                                                       laynor_gamma2,laynor_beta2,embedding_size);</span><br></pre></td></tr></table></figure></div>


            </div>

            
                <div class="post-copyright-info">
                    <div class="article-copyright-info-container">
    <ul>
        <li><strong>Title:</strong> FasterTransformer</li>
        <li><strong>Author:</strong> Tong</li>
        <li><strong>Created at:</strong> 2024-01-27 15:49:02</li>
        
            <li>
                <strong>Updated at:</strong> 2024-08-15 15:18:47
            </li>
        
        <li>
            <strong>Link:</strong> https://github.com/Tong-Cao/Tong-Cao.github.io.git/2024/01/27/FasterTransformer/
        </li>
        <li>
            <strong>License:</strong> This work is licensed under <a class="license" target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">CC BY-NC-SA 4.0</a>.
        </li>
    </ul>
</div>

                </div>
            

            
                <ul class="post-tags-box">
                    
                        <li class="tag-item">
                            <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">#深度学习</a>&nbsp;
                        </li>
                    
                        <li class="tag-item">
                            <a href="/tags/transformer/">#transformer</a>&nbsp;
                        </li>
                    
                </ul>
            

            

            
                <div class="article-nav">
                    
                        <div class="article-prev">
                            <a class="prev"
                            rel="prev"
                            href="/2024/01/27/Transformer/"
                            >
                                <span class="left arrow-icon flex-center">
                                    <i class="fa-solid fa-chevron-left"></i>
                                </span>
                                <span class="title flex-center">
                                    <span class="post-nav-title-item">Transformer</span>
                                    <span class="post-nav-item">Prev posts</span>
                                </span>
                            </a>
                        </div>
                    
                    
                        <div class="article-next">
                            <a class="next"
                            rel="next"
                            href="/2024/01/15/cuda%E7%AC%94%E8%AE%B0/"
                            >
                                <span class="title flex-center">
                                    <span class="post-nav-title-item">cuda笔记</span>
                                    <span class="post-nav-item">Next posts</span>
                                </span>
                                <span class="right arrow-icon flex-center">
                                    <i class="fa-solid fa-chevron-right"></i>
                                </span>
                            </a>
                        </div>
                    
                </div>
            


            
                <div class="comment-container">
                    <div class="comments-container">
    <div id="comment-anchor"></div>
    <div class="comment-area-title">
        <i class="fa-solid fa-comments"></i>&nbsp;Comments
    </div>
    

        
            
 
    <div id="waline"></div>
    <script type="module"  data-pjax>
        import { init } from 'https://evan.beee.top/js/waline.mjs';

        function loadWaline() {
            init({
                el: '#waline',
                serverURL: 'https://example.example.com',
                lang: 'zh-CN',
                dark: 'body[class~="dark-mode"]',
                requiredMeta: ['nick','mail'], // cannot customize by theme config, change it yourself
            });
        }

        if ('true') {
            const loadWalineTimeout = setTimeout(() => {
                loadWaline();
                clearTimeout(loadWalineTimeout);
            }, 1000);
        } else {
            window.addEventListener('DOMContentLoaded', loadWaline);
        }
        
    </script>



        
    
</div>

                </div>
            
        </div>

        
            <div class="toc-content-container">
                <div class="post-toc-wrap">
    <div class="post-toc">
        <div class="toc-title">On this page</div>
        <div class="page-title">FasterTransformer</div>
        <ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#FasterTransformer"><span class="nav-text">FasterTransformer</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B"><span class="nav-text">多头注意力</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%80%E3%80%81linear"><span class="nav-text">一、linear</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BA%8C%E3%80%81Matmul"><span class="nav-text">二、Matmul</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%89%E3%80%81softmax"><span class="nav-text">三、softmax</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9B%9B%E3%80%81Matmul-%E8%AE%A1%E7%AE%97-V"><span class="nav-text">四、Matmul 计算 *V</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BA%94%E3%80%81%E8%BE%93%E5%87%BA%E7%BA%BF%E6%80%A7%E5%B1%82"><span class="nav-text">五、输出线性层</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#FFN"><span class="nav-text">FFN</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%80%E3%80%81%E7%AC%AC%E4%B8%80%E5%B1%82%E7%BA%BF%E6%80%A7%E5%8F%98%E6%8D%A2"><span class="nav-text">一、第一层线性变换</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BA%8C%E3%80%81add-bias-act"><span class="nav-text">二、add_bias_act</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%89%E3%80%81%E7%AC%AC%E4%BA%8C%E5%B1%82%E7%BA%BF%E6%80%A7%E5%8F%98%E6%8D%A2"><span class="nav-text">三、第二层线性变换</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9B%9B%E3%80%81add-bias-input-layernorm"><span class="nav-text">四、add_bias_input_layernorm</span></a></li></ol></li></ol></li></ol>

    </div>
</div>
            </div>
        
    </div>
</div>


                

            </div>
            
            

        </div>

        <div class="main-content-footer">
            <footer class="footer">
    <div class="info-container">
        <div class="copyright-info">
            &copy;
            
              <span>2023</span>
              -
            
            2024&nbsp;&nbsp;<i class="fa-solid fa-heart fa-beat" style="--fa-animation-duration: 0.5s; color: #f54545"></i>&nbsp;&nbsp;<a href="/">Tong</a>
        </div>
        
            <script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
            <div class="website-count info-item">
                
                    <span id="busuanzi_container_site_uv" class="busuanzi_container_site_uv">
                        VISITOR COUNT&nbsp;<span id="busuanzi_value_site_uv" class="busuanzi_value_site_uv"></span>
                    </span>
                
                
                    <span id="busuanzi_container_site_pv" class="busuanzi_container_site_pv">
                        TOTAL PAGE VIEWS&nbsp;<span id="busuanzi_value_site_pv" class="busuanzi_value_site_pv"></span>
                    </span>
                
            </div>
        
        <div class="theme-info info-item">
            <span class="powered-by-container">POWERED BY <?xml version="1.0" encoding="utf-8"?><!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"><svg version="1.1" id="圖層_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" width="1rem" height="1rem" viewBox="0 0 512 512" enable-background="new 0 0 512 512" xml:space="preserve"><path fill="#0E83CD" d="M256.4,25.8l-200,115.5L56,371.5l199.6,114.7l200-115.5l0.4-230.2L256.4,25.8z M349,354.6l-18.4,10.7l-18.6-11V275H200v79.6l-18.4,10.7l-18.6-11v-197l18.5-10.6l18.5,10.8V237h112v-79.6l18.5-10.6l18.5,10.8V354.6z"/></svg><a target="_blank" href="https://hexo.io">Hexo</a></span>
                <br>
            <span class="theme-version-container">THEME&nbsp;<a class="theme-version" target="_blank" href="https://github.com/EvanNotFound/hexo-theme-redefine">Redefine v2.1.3</a>
        </div>
        
        
        
            <div id="start_div" style="display:none">
                2023/4/30 12:00:00
            </div>
            <div>
                Blog up for <span class="odometer" id="runtime_days" ></span> days <span class="odometer" id="runtime_hours"></span> hrs <span class="odometer" id="runtime_minutes"></span> Min <span class="odometer" id="runtime_seconds"></span> Sec
            </div>
        
        
        
            <script async data-pjax>
                try {
                    function odometer_init() {
                    const elements = document.querySelectorAll('.odometer');
                    elements.forEach(el => {
                        new Odometer({
                            el,
                            format: '( ddd).dd',
                            duration: 200
                        });
                    });
                    }
                    odometer_init();
                } catch (error) {}
            </script>
        
        
        
    </div>  
</footer>
        </div>
    </div>

    
        <div class="post-tools">
            <div class="post-tools-container">
    <ul class="article-tools-list">
        <!-- TOC aside toggle -->
        
            <li class="right-bottom-tools page-aside-toggle">
                <i class="fa-regular fa-outdent"></i>
            </li>
        

        <!-- go comment -->
        
            <li class="go-comment">
                <i class="fa-regular fa-comments"></i>
            </li>
        
    </ul>
</div>

        </div>
    

    <div class="right-side-tools-container">
        <div class="side-tools-container">
    <ul class="hidden-tools-list">
        <li class="right-bottom-tools tool-font-adjust-plus flex-center">
            <i class="fa-regular fa-magnifying-glass-plus"></i>
        </li>

        <li class="right-bottom-tools tool-font-adjust-minus flex-center">
            <i class="fa-regular fa-magnifying-glass-minus"></i>
        </li>

        <li class="right-bottom-tools tool-expand-width flex-center">
            <i class="fa-regular fa-expand"></i>
        </li>

        <li class="right-bottom-tools tool-dark-light-toggle flex-center">
            <i class="fa-regular fa-moon"></i>
        </li>

        <!-- rss -->
        

        

        <li class="right-bottom-tools tool-scroll-to-bottom flex-center">
            <i class="fa-regular fa-arrow-down"></i>
        </li>
    </ul>

    <ul class="visible-tools-list">
        <li class="right-bottom-tools toggle-tools-list flex-center">
            <i class="fa-regular fa-cog fa-spin"></i>
        </li>
        
            <li class="right-bottom-tools tool-scroll-to-top flex-center">
                <i class="arrow-up fas fa-arrow-up"></i>
                <span class="percent"></span>
            </li>
        
        
    </ul>
</div>

    </div>

    <div class="image-viewer-container">
    <img src="">
</div>


    


</main>



<script src="//npm.elemecdn.com/hexo-theme-redefine@2.1.3/source/js/utils.js"></script><script src="//npm.elemecdn.com/hexo-theme-redefine@2.1.3/source/js/main.js"></script><script src="//npm.elemecdn.com/hexo-theme-redefine@2.1.3/source/js/layouts/navbarShrink.js"></script><script src="//npm.elemecdn.com/hexo-theme-redefine@2.1.3/source/js/tools/scrollTopBottom.js"></script><script src="//npm.elemecdn.com/hexo-theme-redefine@2.1.3/source/js/tools/lightDarkSwitch.js"></script>




    <script src="//npm.elemecdn.com/hexo-theme-redefine@2.1.3/source/js/tools/codeBlock.js"></script>



    <script src="//npm.elemecdn.com/hexo-theme-redefine@2.1.3/source/js/layouts/lazyload.js"></script>



    <script src="//npm.elemecdn.com/hexo-theme-redefine@2.1.3/source/js/tools/runtime.js"></script>
    <script src="//npm.elemecdn.com/hexo-theme-redefine@2.1.3/source/js/layouts/odometer.min.js"></script>
    <link rel="stylesheet" href="//npm.elemecdn.com/hexo-theme-redefine@2.1.3/source/assets/odometer-theme-minimal.css">



  <script src="//npm.elemecdn.com/hexo-theme-redefine@2.1.3/source/js/libs/Typed.min.js"></script>
  <script src="//npm.elemecdn.com/hexo-theme-redefine@2.1.3/source/js/plugins/typed.js"></script>







<div class="post-scripts pjax">
    
        <script src="//npm.elemecdn.com/hexo-theme-redefine@2.1.3/source/js/tools/tocToggle.js"></script><script src="//npm.elemecdn.com/hexo-theme-redefine@2.1.3/source/js/libs/anime.min.js"></script><script src="//npm.elemecdn.com/hexo-theme-redefine@2.1.3/source/js/layouts/toc.js"></script><script src="//npm.elemecdn.com/hexo-theme-redefine@2.1.3/source/js/plugins/tabs.js"></script>
    
</div>


    <script src="//npm.elemecdn.com/hexo-theme-redefine@2.1.3/source/js/libs/pjax.min.js"></script>
<script>
    window.addEventListener('DOMContentLoaded', () => {
        window.pjax = new Pjax({
            selectors: [
                'head title',
                '.page-container',
                '.pjax',
            ],
            history: true,
            debug: false,
            cacheBust: false,
            timeout: 0,
            analytics: false,
            currentUrlFullReload: false,
            scrollRestoration: false,
            // scrollTo: true,
        });

        document.addEventListener('pjax:send', () => {
            Global.utils.pjaxProgressBarStart();
        });

        document.addEventListener('pjax:complete', () => {
            Global.utils.pjaxProgressBarEnd();
            window.pjax.executeScripts(document.querySelectorAll('script[data-pjax], .pjax script'));
            Global.refresh();
        });
    });
</script>




</body>
</html>
